{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import collections\n",
    "import numpy as np\n",
    "from ppo_torch import *\n",
    "from utils import plotLearning\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading models ...\n"
     ]
    },
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1764/116394235.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mload_checkpoint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_models\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mscore_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpklObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Reinforcement Learning\\My codes\\PPO\\ppo_torch.py\u001b[0m in \u001b[0;36mload_models\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpklObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpklFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmemory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpkl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpklObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mchoose_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    env_name = 'CartPole-v0'\n",
    "    env =gym.make(env_name)\n",
    "    \n",
    "    directory = 'D:\\Reinforcement Learning\\My codes\\PPO/'\n",
    "    \n",
    "    if (not os.path.exists(directory+'models/'+str(env_name)+'/')):\n",
    "        os.makedirs(directory+'models/'+str(env_name)+'/')\n",
    "    if (not os.path.exists(directory + 'plots/'+str(env_name)+'/')):\n",
    "        os.makedirs(directory+'plots/'+str(env_name)+'/')\n",
    "    if (not os.path.exists(directory+'pickle/'+str(env_name)+'/')):\n",
    "        os.makedirs(directory+'pickle/'+str(env_name)+'/')\n",
    "    \n",
    "    N=20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    \n",
    "    n_games = 10\n",
    "    \n",
    "    figure_file = directory+'plots/'+str(env_name)+'/'+env_name +'.png'\n",
    "    pickle_file  = directory+'pickle/'+str(env_name)+'/'\n",
    "    model_file  =directory+'models/'+str(env_name)+'/'\n",
    "    \n",
    "    \n",
    "    agent = Agent(n_actions= env.action_space.n, batch_size = batch_size,\n",
    "                  alpha = alpha, n_epochs=n_epochs, \n",
    "                  input_dims= env.observation_space.shape, pickle_file=pickle_file, model_file=model_file)\n",
    "    \n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "    \n",
    "    best_avg_score = -np.inf\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "    prev_n_games= 0 \n",
    "    \n",
    "    load_checkpoint = True\n",
    "    train = True\n",
    "    \n",
    "    if load_checkpoint:\n",
    "        agent.load_models()\n",
    "        \n",
    "        score_history = pkl.load(agent.pklObj)\n",
    "        prev_n_games = pkl.load(agent.pklObj)\n",
    "        n_steps = pkl.load(agent.pklObj)\n",
    "        learn_iters = pkl.load(agent.pklObj)\n",
    "    \n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            \n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            \n",
    "            n_steps+=1\n",
    "            \n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N ==0:\n",
    "                if train:\n",
    "                    agent.learn()\n",
    "                    learn_iters += 1\n",
    "                \n",
    "            observation = observation_\n",
    "            \n",
    "        score_history.append(score)\n",
    "        \n",
    "        avg_score = np.mean(score_history[-20:])\n",
    "            \n",
    "        if train or (train and (i == n_games-1)):    \n",
    "            if avg_score > best_avg_score:\n",
    "                best_avg_score = avg_score\n",
    "                agent.save_models()\n",
    "                \n",
    "                pkl.dump(score_history, agent.pklObj)\n",
    "                pkl.dump(i, agent.pklObj)\n",
    "                pkl.dump(n_steps, agent.pklObj)\n",
    "                pkl.dump(learn_iters, agent.pklObj)\n",
    "            \n",
    "                \n",
    "        print('Episode ',i , 'Score %.1f' %score, 'Avg score %.1f' %avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "            \n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plotLearning(x= x,scores= score_history, filename= figure_file, window=20)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c34fa1ef26f870bf898be3f80151dfc7cfd794b7630c015fb4b60f1cf4ddf8a7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
